{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-Lab Tutorial: Working Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Reinforcement Learning Lab! This is an introductory tutorial for you to familiarize yourself with OpenAI Gym and the first environment for the exercises.\n",
    "\n",
    "## OpenAI Gym environments\n",
    "\n",
    "The environment **Dangerous GridWorld** is visible in the following figure.\n",
    "\n",
    "<img src=\"images/environment_1.png\" width=\"400\">\n",
    "\n",
    "The agent starts in cell $0$ and has to reach cell $48$, while the cells with the *skull* are the dangerous cells that cause the agent to lose the game. \n",
    "The grey cells represent walls that the robot can not cross. \n",
    "The robot can move in $4$ directions: *LEFT*, *RIGHT*, *UP*, and *DOWN*. \n",
    "\n",
    "However, the robot **doesn't work very well!** It will follow the commands only *90%* of the time. In the other *10%*, it will perform a random action by selecting from the available options, be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the environment, we need first to import the packages of OpenAI Gym. \n",
    "Notice that, due to the structure of this repository, we need to add the parent directory to the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)\n",
    "\n",
    "from DangerousGridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than we can generate a new enviromnent **Dangerous GridWorld** and render it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] [ ] [ ] [ ] [ ] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[W] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [ ] [ ] [ ] [ ] [G] \n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The render is a matrix with cells of different type:\n",
    "* *S* - Start Cell\n",
    "* *W* - Wall Cells\n",
    "* *X* - Death Cells\n",
    "\n",
    "An environment has some useful variables:\n",
    "* *action_space* - number of possible actions (i.e., $4$)]\n",
    "* *observation_space* - space of possible observations (states): usually a range of integers  (i.e., $50$)\n",
    "* *actions* - mapping between action ids and their descriptions\n",
    "* *startstate* - start state (unique)\n",
    "* *goalstate* - goal state (unique)\n",
    "\n",
    "In **Dangerous GridWorld** we have 4 different possible actions numbered from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print( env.action_space )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they are *Left, Right, Up, Down*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n"
     ]
    }
   ],
   "source": [
    "print( env.actions )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States are numbered from 0 to 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print( env.observation_space )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some methods:\n",
    "* *render()* - renders the environment\n",
    "* *pos_to_state(x, y)* - returns the state id given its position in  and  coordinates\n",
    "* *state_to_pos(state)* - returns the coordinates  given a state id\n",
    "* *is_terminal(state)* - returns True if the given *state* is terminal (goal or death)\n",
    "* *evaluate_policy(policy)* - return the average cumulative reward of 10 runs following the given policy\n",
    "* *render_policy()* - renders the policy, showing the selected action for each cell \n",
    "\n",
    "For example, if we want to know the ids and positions for both the start and goal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start id: 0\tGoal id: 48\n",
      "Start position: (0, 0)\tGoal position: (6, 6)\n",
      "Id of state (0, 3): 3\n",
      "\n",
      "[S] [ ] [ ] [ ] [ ] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[W] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [ ] [ ] [ ] [ ] [G] \n"
     ]
    }
   ],
   "source": [
    "start = env.start_state\n",
    "goal = env.goal_state\n",
    "print(\"Start id: {}\\tGoal id: {}\".format(start, goal))\n",
    "print(\"Start position: {}\\tGoal position: {}\".format(env.state_to_pos(start), env.state_to_pos(goal)))\n",
    "print(\"Id of state (0, 3): {}\".format(env.pos_to_state(3, 0)))\n",
    "print()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it can be necessary to know if a state is **terminal**. In general, the **goal state** and the death states are terminal. Using the *is_terminal(state)* function is a fast method to obtain this information. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state 1 ((1, 0)), is terminal? False\n",
      "The state 6 ((6, 0)), is terminal? True\n",
      "The state 48 ((6, 6)), is terminal? True\n"
     ]
    }
   ],
   "source": [
    "is_1_terminal = env.is_terminal(1)\n",
    "is_6_terminal = env.is_terminal(6)\n",
    "is_48_terminal = env.is_terminal(48)\n",
    "\n",
    "print( f\"The state 1 ({env.state_to_pos(1)}), is terminal? {is_1_terminal}\" )\n",
    "print( f\"The state 6 ({env.state_to_pos(6)}), is terminal? {is_6_terminal}\" )\n",
    "print( f\"The state 48 ({env.state_to_pos(48)}), is terminal? {is_48_terminal}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Methods: *sample()* vs *transition_prob()*\n",
    "\n",
    "In **Dangerous GridWorld**, there are two key methods to navigate the environment:\n",
    "* *sample(state, action)* - returns a new state sampled from the ones that can be reached from *state* by performing *action*, both given as ids\n",
    "* *transition_prob(state, action, next_state)* - returns the probability of reaching the state *next_state*, starting from *state* and selecting the action *action*\n",
    "\n",
    "In some cases, we want to analyze only the transition table (e.g., policy/value iteration) so we can use the function **transition_prob** to obtain the probability of reaching a state. In some other cases, we want to actually move the agent in the environment (e.g., MC tree-search or testing phase), and we use the function **sample** to try to execute the action and see what happens *(remember, the robot will follow your instructions only 90% of the time!)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of the method **transition_prob(state, action, new_state)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting D (DOWN)? 0.9\n",
      "What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting R (RIGHT)? 0.03\n",
      "What's the probability of ending up in state 48 (6, 6) starting from state 0 (0, 0) and selecting R (RIGHT)? 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\"What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting D (DOWN)? {env.transition_prob(0, 3, 7)}\" )\n",
    "print( f\"What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting R (RIGHT)? {env.transition_prob(0, 1, 7)}\" )\n",
    "print( f\"What's the probability of ending up in state 48 (6, 6) starting from state 0 (0, 0) and selecting R (RIGHT)? {env.transition_prob(0, 1, 48)}\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of the method **sample**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (0, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (0, 1)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n"
     ]
    }
   ],
   "source": [
    "start_position = 0\n",
    "action = 1\n",
    "\n",
    "for _ in range(10):\n",
    "    new_state = env.sample(action, start_position) \n",
    "    print( f\"Starting from {env.state_to_pos(start_position)} and performing action {env.actions[0]}, the reobot ends up in state: {env.state_to_pos(new_state)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Methods: *sample_episode()*\n",
    "\n",
    "In **Dangerous GridWorld**, there is a method to sample a full trajectory following a given stochastic policy:\n",
    "* *sample_episode( policy )*: returns an array of N elements (the number of steps), where each element is an array of 3 values *<state, action, reward>*.\n",
    "\n",
    "The policy should be an array of N elements, one for each state, where each element is an array of A elements, where A is the number of possible actions from the state. This sequence of values represents the probability distribution over the actions. \n",
    "\n",
    "Supposing a uniform policy, where each action has the same probability of being selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [[1 / env.action_space for _ in range(env.action_space)] for _ in range(env.observation_space)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, from the state 0, we have the following (*uniform*) distribution over the actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution from state 0: [0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "print( \"Distribution from state 0:\", policy[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a complete episode starting from state **44**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44, 0, -0.1], [43, 2, -0.1], [43, 0, -0.1], [42, 2, -0.1], [35, 0, -0.1], [35, 0, -0.1], [35, 1, -0.1], [35, 3, -0.1], [42, 3, -0.1], [42, 0, -0.1], [42, 1, -0.1], [43, 2, -0.1], [43, 1, -0.1], [44, 0, -0.1], [43, 3, -0.1], [43, 3, -0.1], [43, 3, -0.1], [43, 1, -0.1], [44, 0, -0.1], [43, 1, -0.1], [44, 2, -0.1], [44, 2, -0.1], [44, 1, -0.1], [45, 3, -0.1], [45, 1, -0.1], [46, 3, -0.1]]\n"
     ]
    }
   ],
   "source": [
    "trajectory = env.sample_episode( policy )\n",
    "print( trajectory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each array element represents a step of the episode in the tuple *<state, action, reward>*. For example, the first element means that starting from state **44** and performing the action **0 (LEFT)**, the robot obtains a reward of **-0.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b4a0bc6cf61cb63657bef9bf4f66287d0630dbbd28c8eb6c57e9ede2e775d87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
